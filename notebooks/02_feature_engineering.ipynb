# notebooks/02_feature_engineering.ipynb

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ForecastEngine: Feature Engineering Experiments\n",
    "\n",
    "This notebook experiments with different feature engineering approaches for time-series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from feature_engineering.feature_builder import FeatureBuilder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Basic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('../data/sample_data.csv')\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "print(f\"Original data shape: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature builder with different configurations\n",
    "config_basic = {\n",
    "    'lags': {'periods': [1, 7]},\n",
    "    'rolling_windows': {'windows': [7], 'functions': ['mean']},\n",
    "    'seasonality': {'enabled': True}\n",
    "}\n",
    "\n",
    "config_advanced = {\n",
    "    'lags': {'periods': [1, 2, 3, 7, 14, 30]},\n",
    "    'rolling_windows': {'windows': [7, 14, 30], 'functions': ['mean', 'std']},\n",
    "    'seasonality': {'enabled': True}\n",
    "}\n",
    "\n",
    "# Build features with basic config\n",
    "fb_basic = FeatureBuilder(config_basic)\n",
    "features_basic = fb_basic.build_features(data, 'value', 'date', ['price', 'promotion', 'temperature'])\n",
    "\n",
    "print(f\"Basic features shape: {features_basic.shape}\")\n",
    "print(f\"Basic feature columns: {list(features_basic.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "def prepare_modeling_data(features_df, target_col='value'):\n",
    "    feature_cols = [col for col in features_df.columns \n",
    "                   if col not in [target_col, 'date'] and not col.startswith('Unnamed')]\n",
    "    \n",
    "    X = features_df[feature_cols]\n",
    "    y = features_df[target_col]\n",
    "    \n",
    "    # Split into train/test\n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, feature_cols\n",
    "\n",
    "# Analyze basic features\n",
    "X_train_basic, X_test_basic, y_train_basic, y_test_basic, feature_cols_basic = prepare_modeling_data(features_basic)\n",
    "\n",
    "# Train model and get feature importance\n",
    "rf_basic = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_basic.fit(X_train_basic, y_train_basic)\n",
    "\n",
    "# Feature importance\n",
    "importance_basic = pd.DataFrame({\n",
    "    'feature': feature_cols_basic,\n",
    "    'importance': rf_basic.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importance_basic.head(10), x='importance', y='feature')\n",
    "plt.title('Top 10 Feature Importance (Basic Features)')\n",
    "plt.xlabel('Importance')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 most important features:\")\n",
    "print(importance_basic.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build advanced features\n",
    "fb_advanced = FeatureBuilder(config_advanced)\n",
    "features_advanced = fb_advanced.build_features(data, 'value', 'date', ['price', 'promotion', 'temperature'])\n",
    "\n",
    "print(f\"Advanced features shape: {features_advanced.shape}\")\n",
    "print(f\"Number of features: {len([col for col in features_advanced.columns if col not in ['value', 'date']])}\")\n",
    "\n",
    "# Compare performance\n",
    "X_train_adv, X_test_adv, y_train_adv, y_test_adv, feature_cols_adv = prepare_modeling_data(features_advanced)\n",
    "\n",
    "rf_advanced = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_advanced.fit(X_train_adv, y_train_adv)\n",
    "\n",
    "# Predictions and performance\n",
    "pred_basic = rf_basic.predict(X_test_basic)\n",
    "pred_advanced = rf_advanced.predict(X_test_adv)\n",
    "\n",
    "mae_basic = mean_absolute_error(y_test_basic, pred_basic)\n",
    "mae_advanced = mean_absolute_error(y_test_adv, pred_advanced)\n",
    "\n",
    "print(f\"\\nPerformance Comparison:\")\n",
    "print(f\"Basic features MAE: {mae_basic:.3f}\")\n",
    "print(f\"Advanced features MAE: {mae_advanced:.3f}\")\n",
    "print(f\"Improvement: {((mae_basic - mae_advanced) / mae_basic * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom features\n",
    "def create_custom_features(df):\n",
    "    \"\"\"Create domain-specific custom features\"\"\"\n",
    "    df_custom = df.copy()\n",
    "    \n",
    "    # Price-value ratio\n",
    "    df_custom['price_value_ratio'] = df_custom['price'] / df_custom['value']\n",
    "    \n",
    "    # Temperature bins\n",
    "    df_custom['temp_cold'] = (df_custom['temperature'] < 20).astype(int)\n",
    "    df_custom['temp_hot'] = (df_custom['temperature'] > 30).astype(int)\n",
    "    \n",
    "    # Interaction features\n",
    "    df_custom['promotion_temp'] = df_custom['promotion'] * df_custom['temperature']\n",
    "    df_custom['price_promotion'] = df_custom['price'] * df_custom['promotion']\n",
    "    \n",
    "    # Momentum features\n",
    "    df_custom['value_momentum_3'] = df_custom['value'].rolling(3).apply(lambda x: x.iloc[-1] - x.iloc[0])\n",
    "    df_custom['value_acceleration'] = df_custom['value_momentum_3'].diff()\n",
    "    \n",
    "    # Volatility features\n",
    "    df_custom['value_volatility_7'] = df_custom['value'].rolling(7).std()\n",
    "    df_custom['high_volatility'] = (df_custom['value_volatility_7'] > df_custom['value_volatility_7'].median()).astype(int)\n",
    "    \n",
    "    return df_custom\n",
    "\n",
    "# Apply custom features\n",
    "features_custom = create_custom_features(features_advanced)\n",
    "features_custom = features_custom.dropna()\n",
    "\n",
    "print(f\"Custom features shape: {features_custom.shape}\")\n",
    "print(f\"New custom features added: {len(features_custom.columns) - len(features_advanced.columns)}\")\n",
    "\n",
    "# Test custom features\n",
    "X_train_custom, X_test_custom, y_train_custom, y_test_custom, feature_cols_custom = prepare_modeling_data(features_custom)\n",
    "\n",
    "rf_custom = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_custom.fit(X_train_custom, y_train_custom)\n",
    "\n",
    "pred_custom = rf_custom.predict(X_test_custom)\n",
    "mae_custom = mean_absolute_error(y_test_custom, pred_custom)\n",
    "\n",
    "print(f\"\\nCustom features MAE: {mae_custom:.3f}\")\n",
    "print(f\"Improvement over advanced: {((mae_advanced - mae_custom) / mae_advanced * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for custom features\n",
    "importance_custom = pd.DataFrame({\n",
    "    'feature': feature_cols_custom,\n",
    "    'importance': rf_custom.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Select top features\n",
    "top_features = importance_custom.head(15)['feature'].tolist()\n",
    "\n",
    "print(f\"Selected top {len(top_features)} features:\")\n",
    "print(top_features)\n",
    "\n",
    "# Test with selected features only\n",
    "X_train_selected = X_train_custom[top_features]\n",
    "X_test_selected = X_test_custom[top_features]\n",
    "\n",
    "rf_selected = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_selected.fit(X_train_selected, y_train_custom)\n",
    "\n",
    "pred_selected = rf_selected.predict(X_test_selected)\n",
    "mae_selected = mean_absolute_error(y_test_custom, pred_selected)\n",
    "\n",
    "print(f\"\\nSelected features MAE: {mae_selected:.3f}\")\n",
    "print(f\"Performance vs all features: {((mae_custom - mae_selected) / mae_custom * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "results = pd.DataFrame({\n",
    "    'Approach': ['Basic Features', 'Advanced Features', 'Custom Features', 'Selected Features'],\n",
    "    'MAE': [mae_basic, mae_advanced, mae_custom, mae_selected],\n",
    "    'Num_Features': [len(feature_cols_basic), len(feature_cols_adv), len(feature_cols_custom), len(top_features)]\n",
    "})\n",
    "\n",
    "results['Improvement'] = ((results['MAE'].iloc[0] - results['MAE']) / results['MAE'].iloc[0] * 100).round(1)\n",
    "\n",
    "print(\"=== FEATURE ENGINEERING RESULTS ===\")\n",
    "print(results)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# MAE comparison\n",
    "axes[0].bar(results['Approach'], results['MAE'], color=['blue', 'green', 'orange', 'red'])\n",
    "axes[0].set_title('MAE by Feature Engineering Approach')\n",
    "axes[0].set_ylabel('Mean Absolute Error')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Feature count vs performance\n",
    "axes[1].scatter(results['Num_Features'], results['MAE'], s=100, c=['blue', 'green', 'orange', 'red'])\n",
    "for i, txt in enumerate(results['Approach']):\n",
    "    axes[1].annotate(txt, (results['Num_Features'].iloc[i], results['MAE'].iloc[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "axes[1].set_xlabel('Number of Features')\n",
    "axes[1].set_ylabel('Mean Absolute Error')\n",
    "axes[1].set_title('Feature Count vs Performance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== KEY INSIGHTS ===\")\n",
    "print(f\"Best performing approach: {results.loc[results['MAE'].idxmin(), 'Approach']}\")\n",
    "print(f\"Best MAE: {results['MAE'].min():.3f}\")\n",
    "print(f\"Total improvement: {results['Improvement'].max():.1f}%\")\n",
    "print(f\"Optimal feature count: {results.loc[results['MAE'].idxmin(), 'Num_Features']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}